# åç«¯æµ‹è¯•è¯´æ˜

å¿«é€Ÿæµ‹è¯•è„šæœ¬ç”¨äºä½¿ç”¨ mock LLM æ¥å¿«é€ŸéªŒè¯ benchmark ç±»çš„åŠŸèƒ½ï¼Œæ— éœ€åŠ è½½çœŸå®æ¨¡å‹æˆ–è°ƒç”¨ APIã€‚

## æ¦‚è¿°

è¯¥æµ‹è¯•å¥—ä»¶åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

### ğŸ“ æ–‡ä»¶ç»“æ„

```
tests/
â”œâ”€â”€ __init__.py                      # åŒ…åˆå§‹åŒ–æ–‡ä»¶
â”œâ”€â”€ test_benchmarks.py               # å®Œæ•´çš„æµ‹è¯•è„šæœ¬ï¼ˆæ”¯æŒè¯¦ç»†é€‰é¡¹ï¼‰
â”œâ”€â”€ quick_test.py                    # å¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
â”œâ”€â”€ fixtures/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mock_llm.py                  # Mock LLM å®ç°
â”‚   â””â”€â”€ dataset_setup.py             # æµ‹è¯•æ•°æ®é›†è®¾ç½®
â””â”€â”€ test_data/                       # æµ‹è¯•æ•°æ®ç›®å½•ï¼ˆè¿è¡Œæ—¶ç”Ÿæˆï¼‰
    â”œâ”€â”€ LongBench/
    â”œâ”€â”€ LongBenchV2/
    â””â”€â”€ C-Eval/
```

## ç»„ä»¶è¯´æ˜

### 1. Mock LLM (`fixtures/mock_llm.py`)

æä¾›ä»¥ä¸‹ mock ç±»ï¼š

- **MockTokenizer**: æ¨¡æ‹Ÿ `transformers.AutoTokenizer`
  - æ”¯æŒ `__call__()` è¿›è¡Œ tokenization
  - æ”¯æŒ `decode()` è¿›è¡Œåå‘è½¬æ¢
  - è¿”å› mock tensor å¯¹è±¡

- **MockModel**: æ¨¡æ‹Ÿ `transformers.AutoModelForCausalLM`
  - æ”¯æŒ `generate()` æ–¹æ³•ç”Ÿæˆ token
  - æ”¯æŒ `to(device)` ç§»åŠ¨åˆ°ä¸åŒè®¾å¤‡
  - å¿«é€Ÿè¿”å›ç»“æœï¼Œæ— éœ€å®é™…è®¡ç®—

- **MockOpenAIClient**: æ¨¡æ‹Ÿ OpenAI API å®¢æˆ·ç«¯
  - æ”¯æŒ `messages.create()` API è°ƒç”¨
  - è¿”å› mock API å“åº”
  - ç¡®å®šæ€§ç”Ÿæˆå“åº”å†…å®¹

### 2. æ•°æ®é›†è®¾ç½® (`fixtures/dataset_setup.py`)

- **`setup_test_datasets()`**: åˆ›å»º mock æ•°æ®é›†
  - ä¸º LongBench, LongBenchV2, C-Eval åˆ›å»º JSONL æ ¼å¼çš„æµ‹è¯•æ•°æ®
  - æ”¯æŒè‡ªå®šä¹‰æ ·æœ¬æ•°é‡
  - è‡ªåŠ¨ç”Ÿæˆåˆç†çš„æµ‹è¯•æ•°æ®

- **`update_config_for_testing()`**: æ›´æ–°é…ç½®æŒ‡å‘æµ‹è¯•æ•°æ®
  - ä¿®æ”¹ `dataset_paths.json` æŒ‡å‘æµ‹è¯•æ•°æ®ç›®å½•
  - ä¿å­˜åŸå§‹é…ç½®ä¸ºå¤‡ä»½

- **`cleanup_test_data()`**: æ¸…ç†æµ‹è¯•æ•°æ®

### 3. æµ‹è¯•è„šæœ¬

#### å®Œæ•´ç‰ˆ (`test_benchmarks.py`)

åŠŸèƒ½å®Œæ•´çš„æµ‹è¯•è„šæœ¬ï¼Œæ”¯æŒè¯¦ç»†çš„å‘½ä»¤è¡Œé€‰é¡¹ã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```bash
# æµ‹è¯•æ‰€æœ‰ benchmark
python tests/test_benchmarks.py

# æµ‹è¯•ç‰¹å®š benchmark
python tests/test_benchmarks.py --bench LongBench

# æµ‹è¯•æœ¬åœ°æ¨¡å‹è¯„ä¼°
python tests/test_benchmarks.py --mode local

# æµ‹è¯• API æ¨¡å‹è¯„ä¼°
python tests/test_benchmarks.py --mode api

# æµ‹è¯•æœ¬åœ°å’Œ API ä¸¤ç§æ¨¡å¼
python tests/test_benchmarks.py --mode all

# è‡ªå®šä¹‰æ ·æœ¬æ•°é‡
python tests/test_benchmarks.py --samples 10

# ä¿å­˜æµ‹è¯•ç»“æœåˆ°æŒ‡å®šæ–‡ä»¶
python tests/test_benchmarks.py --output my_results.json

# æµ‹è¯•åè‡ªåŠ¨æ¸…ç†æµ‹è¯•æ•°æ®
python tests/test_benchmarks.py --cleanup
```

**å®Œæ•´é€‰é¡¹ï¼š**

```
usage: test_benchmarks.py [-h] [--bench BENCH] [--samples SAMPLES] 
                          [--mode {local,api,all}] [--output OUTPUT] 
                          [--cleanup] [--verbose]

optional arguments:
  -h, --help              Show help message
  --bench BENCH           Specific benchmarker to test (e.g., 'LongBench')
  --samples SAMPLES       Number of samples per subdataset (default: 5)
  --mode {local,api,all}  Test mode: local/api/all (default: local)
  --output OUTPUT         Output file for results (default: test_results.json)
  --cleanup               Clean up test data after tests
  --verbose               Print detailed output
```

#### å¿«é€Ÿç‰ˆ (`quick_test.py`)

ç®€åŒ–ç‰ˆè„šæœ¬ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•å•ä¸ªæˆ–å¤šä¸ª benchmarkã€‚

**åŸºæœ¬ç”¨æ³•ï¼š**

```bash
# æµ‹è¯•æ‰€æœ‰ benchmark
python tests/quick_test.py

# å¿«é€Ÿæµ‹è¯• LongBench
python tests/quick_test.py longbench

# å¿«é€Ÿæµ‹è¯• LongBenchV2
python tests/quick_test.py longbench_v2

# å¿«é€Ÿæµ‹è¯• C-Eval
python tests/quick_test.py c-eval

# è‡ªå®šä¹‰æ ·æœ¬æ•°é‡
python tests/quick_test.py longbench --samples 10
```

## æµ‹è¯•æµç¨‹

### è‡ªåŠ¨æµç¨‹

è„šæœ¬ä¼šè‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1. **ç¯å¢ƒè®¾ç½®**
   - åˆ›å»ºæµ‹è¯•æ•°æ®ç›®å½•
   - ç”Ÿæˆ mock æ•°æ®é›†ï¼ˆJSONL æ ¼å¼ï¼‰
   - æ›´æ–°é…ç½®æ–‡ä»¶æŒ‡å‘æµ‹è¯•æ•°æ®

2. **æµ‹è¯•æ‰§è¡Œ**
   - ä¸ºæ¯ä¸ª benchmark åˆ›å»º mock æ¨¡å‹å’Œå®¢æˆ·ç«¯
   - è°ƒç”¨ `evaluate_local_llm()` æµ‹è¯•æœ¬åœ°æ¨¡å‹è¯„ä¼°
   - è°ƒç”¨ `evaluate_api_llm()` æµ‹è¯• API æ¨¡å‹è¯„ä¼°
   - æ”¶é›†ç»“æœ

3. **ç»“æœè¾“å‡º**
   - æ‰“å°è¯¦ç»†çš„æµ‹è¯•æ‘˜è¦
   - ä¿å­˜ JSON æ ¼å¼çš„æµ‹è¯•ç»“æœ
   - å¯é€‰ï¼šæ¸…ç†æµ‹è¯•æ•°æ®

### æ‰‹åŠ¨æ­¥éª¤

å¦‚æœä½ æƒ³æ‰‹åŠ¨è¿è¡Œæµ‹è¯•ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹æ­¥éª¤ï¼š

```python
from fixtures.mock_llm import create_mock_model, create_mock_tokenizer, create_mock_client
from fixtures.dataset_setup import setup_test_datasets, update_config_for_testing
from bench import init_all_benchmarkers

# 1. è®¾ç½®
setup_test_datasets(num_samples=5)
update_config_for_testing()

# 2. è·å– benchmarker
benchmarkers = init_all_benchmarkers()
benchmarker = benchmarkers["LongBench"]

# 3. æµ‹è¯•æœ¬åœ°æ¨¡å‹
model = create_mock_model()
tokenizer = create_mock_tokenizer()
result = benchmarker.evaluate_local_llm(
    model=model,
    tokenizer=tokenizer,
    subdataset_name="2wikimqa"
)

# 4. æµ‹è¯• API æ¨¡å‹
client = create_mock_client()
result = benchmarker.evaluate_api_llm(
    client=client,
    model="gpt-4",
    subdataset_name="2wikimqa"
)
```

## æ”¯æŒçš„ Benchmark

ç›®å‰æ”¯æŒä»¥ä¸‹ benchmark çš„å¿«é€Ÿæµ‹è¯•ï¼š

### LongBench
- é•¿æ–‡æœ¬ç†è§£è¯„ä¼°
- å­æ•°æ®é›†ï¼š2wikimqa, dureader, gov_report, hotpotqa, narrativeqa, ...
- æµ‹è¯•æ•°æ®åŒ…å«ï¼šquestion, context, answers

### LongBenchV2
- æ”¹è¿›çš„é•¿æ–‡æœ¬ç†è§£è¯„ä¼°
- é¢†åŸŸï¼šCode Repository Understanding, Long Dialogue History, In-context Learning, ...
- æµ‹è¯•æ•°æ®åŒ…å«ï¼šquestion, instruction, context, answer

### C-Eval
- ä¸­æ–‡å¤šå­¦ç§‘è¯„ä¼°ï¼ˆplaceholderï¼‰
- æµ‹è¯•æ•°æ®åŒ…å«ï¼šquestion, options (A/B/C/D), answer

## æµ‹è¯•ç»“æœ

### ç»“æœæ ¼å¼

è„šæœ¬é»˜è®¤å°†ç»“æœä¿å­˜ä¸º `test_results.json`ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```json
{
  "LongBench": {
    "benchmarker": "LongBench",
    "local_tests": {
      "2wikimqa": {
        "dataset": "2wikimqa",
        "model_type": "local",
        "metrics": {
          "total": 5,
          "processed": 5
        },
        "predictions": [...]
      }
    },
    "api_tests": {
      "2wikimqa": {
        "dataset": "2wikimqa",
        "model_type": "api",
        "model": "mock-gpt-4",
        "metrics": {
          "total": 5,
          "processed": 5
        },
        "predictions": [...]
      }
    }
  }
}
```

### è¾“å‡ºç¤ºä¾‹

```
======================================================================
ğŸš€ Setting up test environment
======================================================================

ğŸ“ Setting up mock datasets in: /path/to/tests/test_data

âœ“ Created mock LongBench dataset with 5 subdatasets
âœ“ Created mock LongBenchV2 dataset with 3 domains
âœ“ Created mock C-Eval dataset with 3 subjects

âœ“ All mock datasets created successfully

âœ“ Updated config: /path/to/LLMBenchShower/configs/dataset_paths.json

======================================================================
Testing Benchmarker: LongBench
======================================================================

Available subdatasets: 6
Testing subdataset: 2wikimqa

ğŸ“ Testing local LLM evaluation: LongBench/2wikimqa
âœ“ Local LLM evaluation completed
  - Total samples: 5
  - Processed: 5

ğŸ“¡ Testing API LLM evaluation: LongBench/2wikimqa
âœ“ API LLM evaluation completed
  - Total samples: 5
  - Processed: 5

======================================================================
ğŸ“Š TEST SUMMARY
======================================================================

âœ“ LongBench
  Local LLM tests:
    âœ“ 2wikimqa: 5/5 samples processed
  API LLM tests:
    âœ“ 2wikimqa: 5/5 samples processed

...

âœ“ Testing complete!
```

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆéœ€è¦ mock LLMï¼Ÿ
A: Mock LLM å…è®¸å¿«é€Ÿæµ‹è¯•ä»£ç é€»è¾‘è€Œæ— éœ€ï¼š
- ä¸‹è½½å’ŒåŠ è½½çœŸå®çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆé€šå¸¸éœ€è¦å‡  GB çš„å†…å­˜ï¼‰
- è°ƒç”¨å®é™…çš„ APIï¼ˆéœ€è¦æœ‰æ•ˆçš„ API keyï¼Œå¯èƒ½äº§ç”Ÿè´¹ç”¨ï¼‰
- ç­‰å¾…çœŸå®çš„æ¨ç†æ—¶é—´ï¼ˆå¯èƒ½éœ€è¦æ•°ç§’åˆ°æ•°åˆ†é’Ÿï¼‰

è¿™æ ·å¯ä»¥åœ¨å¼€å‘å’Œ CI/CD æµç¨‹ä¸­å¿«é€Ÿè¿­ä»£ã€‚

### Q: Mock LLM è¿”å›çš„æ˜¯çœŸå®çš„ç­”æ¡ˆå—ï¼Ÿ
A: ä¸æ˜¯ã€‚Mock LLM è¿”å›ç¡®å®šæ€§çš„æ¨¡æ‹Ÿå“åº”ï¼Œç”¨äºæµ‹è¯•ä»£ç æµç¨‹å’Œç»“æœæ ¼å¼ã€‚
è¦è·å¾—çœŸå®çš„è¯„ä¼°ç»“æœï¼Œéœ€è¦ä½¿ç”¨å®é™…çš„æ¨¡å‹ã€‚

### Q: å¦‚ä½•æµ‹è¯•æˆ‘æ–°æ·»åŠ çš„ benchmarkï¼Ÿ
A: 
1. åœ¨ `fixtures/dataset_setup.py` ä¸­æ·»åŠ ä½ çš„ benchmark çš„æ•°æ®ç”Ÿæˆå‡½æ•°
2. åœ¨ `setup_test_datasets()` ä¸­è°ƒç”¨è¯¥å‡½æ•°
3. è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š`python tests/test_benchmarks.py --bench YourBench`

### Q: å¦‚ä½•è‡ªå®šä¹‰ mock æ•°æ®ï¼Ÿ
A: ä¿®æ”¹ `fixtures/dataset_setup.py` ä¸­ç›¸åº”çš„æ•°æ®ç”Ÿæˆå‡½æ•°ã€‚
ä¾‹å¦‚ï¼Œè¦æ”¹å˜æ ·æœ¬æ•°é‡ï¼Œä½¿ç”¨ `--samples` å‚æ•°ã€‚

### Q: æµ‹è¯•ä¼šä¿®æ”¹åŸå§‹é…ç½®å—ï¼Ÿ
A: è„šæœ¬ä¼šä¿®æ”¹ `dataset_paths.json`ï¼Œä½†ä¼šåˆ›å»º `.backup` å¤‡ä»½æ–‡ä»¶ã€‚
ä½ å¯ä»¥æ‰‹åŠ¨æ¢å¤æˆ–åˆ é™¤å¤‡ä»½æ–‡ä»¶åé‡æ–°è¿è¡Œè„šæœ¬ã€‚

## æœ€ä½³å®è·µ

1. **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨ `quick_test.py` å¿«é€ŸéªŒè¯æ–°ä»£ç 
2. **CI/CD æµç¨‹**ï¼šä½¿ç”¨ `test_benchmarks.py` è¿›è¡Œå®Œæ•´æµ‹è¯•
3. **è°ƒè¯•**ï¼šä½¿ç”¨ `--verbose` é€‰é¡¹æŸ¥çœ‹è¯¦ç»†è¾“å‡º
4. **æ¸…ç†**ï¼šä½¿ç”¨ `--cleanup` é€‰é¡¹åœ¨æµ‹è¯•åè‡ªåŠ¨æ¸…ç†

## æ‰©å±•

å¦‚æœéœ€è¦ä¸ºå…¶ä»– benchmark æ·»åŠ æ”¯æŒï¼Œå‚è€ƒä»¥ä¸‹æ­¥éª¤ï¼š

1. **æ·»åŠ  mock æ•°æ®ç”Ÿæˆ**ï¼šåœ¨ `fixtures/dataset_setup.py` ä¸­æ·»åŠ å‡½æ•°
   ```python
   def create_mock_mybench_dataset(test_data_dir: str, num_samples: int = 5):
       # åˆ›å»º JSONL æ ¼å¼çš„æµ‹è¯•æ•°æ®
       pass
   ```

2. **åœ¨ `setup_test_datasets()` ä¸­è°ƒç”¨**ï¼š
   ```python
   create_mock_mybench_dataset(test_data_dir, num_samples)
   ```

3. **æ›´æ–°é…ç½®æ˜ å°„**ï¼ˆå¦‚éœ€è¦ï¼‰ï¼šåœ¨ `update_config_for_testing()` ä¸­æ·»åŠ è·¯å¾„

## è®¸å¯è¯

ä¸ä¸»é¡¹ç›®ç›¸åŒã€‚
